The maximum number of threads running our kernel that can be
simultaneously scheduled for execution is 16,384. This is based on the results of the CUDA Occupancy Calculator as well as the ./deviceQuery info about the GPU. According to the occupancy calculator, our limiting factor is the number of warps available. The result of this is that we are limited to 8 blocks per SM. 8 blocks is (8*256) = 2048 threads which is the maximum allowable on one SM. Since there are 8 SMs, we can schedule 16,384 threads at once. The inputs to our spreadsheet were: 256 threads per block, 21 registers per thread, and 2048 bytes of shared memory per block. 

